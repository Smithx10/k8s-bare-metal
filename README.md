# k8s-bare-metal

Guide for running Kubernetes on Triton using Packer and Terraform.

The initial goal of this guide is to build out the following instances akin to
the Hard Way post but with extended Triton exclusive features.

- 1x `controller` infrastructure container running `kube-apiserver`,
  `kube-controller-manager`, and `kube-scheduler`.
- 1x `worker` running KVM for `kubelet`, `kube-proxy`, and `docker`.
- 1x `edge worker` running KVM for `kubelet`, `kube-proxy`, and `docker`.
- 1x `bastion` node for bridging into the private network.
- `etcd` cluster provided by the Autopilot Pattern

## Dependencies

Install the [Triton CLI tool](https://docs.joyent.com/public-cloud/api-access/cloudapi).

Packer templates in this project require JSON5 support in `packer(1)` or the
`cfgt(1)` utility.

- Usage with unpatched packer: `cfgt -i kvm-edge-worker.json5 | packer build -`
- Usage with patched packer: `packer build kvm-edge-worker.json5`

[packer w/ JSON5 support](https://github.com/sean-/packer/tree/f-json5)
cfgt: `go get -u github.com/sean-/cfgt`

Certificates are created via CloudFlare's PKI toolkit, [`cfssl`](https://cfssl.org/).

I use [`jq`](https://stedolan.github.io/jq/) below for pulling information.

I also use [`direnv`](https://direnv.net/) for storing environment variables used by this project.

## Create a private fabric network

* In the Triton dashboard, create a private fabric network with the following
  configuration.

```json
{
    "id": "00000000-0000-0000-0000-000000000001",
    "name": "fubarnetes",
    "public": false,
    "fabric": true,
    "gateway": "10.20.0.1",
    "internet_nat": true,
    "provision_end_ip": "10.20.0.254",
    "provision_start_ip": "10.20.0.2",
    "resolvers": [
        "8.8.8.8",
        "8.8.4.4"
    ],
    "routes": {},
    "subnet": "10.20.0.0/24",
    "vlan_id": 2
}
```

* Set our private network's UUID as an environment variable to `export PRIVATE_NETWORK=$(triton network get fubarnetes | jq -r .id)`
* We'll also set Joyent's public network UUID to `export PUBLIC_NETWORK=$(triton network get Joyent-SDC-Public | jq -r .id)`

## Create your etcd cluster

This is a great chance to use the Autopilot Pattern etcd project to spin-up a
private etcd cluster for our Kubernetes services.

* Set your new private network to be the default for Docker containers under the
  Joyent Public Cloud Portal. This helps support easily rolling out etcd by
  creating all of our non-public containers in the correct network through
  Docker Compose.
* `git clone git@github.com:autopilotpattern/etcd.git && cd etcd` and run
  `./start.sh`
* Your cluster should bootstrap on its own.
* Note your cluster IP addresses, I use the following rather obtuse line of
  shell.

```sh
$ triton-docker inspect $(triton-docker ps --format "{{.Names}}" --filter 'name=e_etcd') | jq -r '.[].NetworkSettings.IPAddress'
```

## Create Kubernetes images using Packer

We'll first create a bastion for securing our build environment, then build an
image for each part of our Kubernetes cluster.

1. Run `make build/bastion` to build a bastion image.
1. `triton create --wait --name=fubarnetes --network=Joyent-SDC-Public,fubarnetes -m user-data="hostname=fubarnetes" k8s-bastion-lx-16.04 14ad9d54`
1. Grab the IP address of your bastion instance and set to `export BASTION_HOST` env var.
1. Use bastion instance to build remaining images on your private fabric network.
1. Run `make build/controller build/edge build/worker`

## Provision infrastructure using Terraform

1. Create input variables for Terraform within `.terraform.vars` by copying the
   sample `.terraform.vars.example` file.
1. Configure infrastructure using Terraform `terraform plan`
1. `terraform apply`

## Upload assets and restart the cluster

After we've created our infrastructure we're left with a few files that need to
be uploaded to our nodes. We'll use a simple shell script to upload them then
restart cluster services. We do this through the bastion instance.

Since the configuration files and TLS certs are automatically generated, there's
nothing to do but run scp.

...

## Notes

- Generated JSON files in this project are treated as ephemeral build
  assets. They're deleted during most `make build` steps.
- When debugging packer use `PACKER_LOG=1 make build/worker`.
- When working with ubuntu images on Triton, please note the difference between
  [default SSH users][default]. On ubuntu certified images the default user is
  `ubuntu`, on Joyent built ubuntu images use `root`.

[default]: https://github.com/joyent/node-triton/issues/3#issuecomment-136519245

## Configuration

The following is a list of configuration files required/generated by the
process. Most of these are generated for you by Terraform.

### /var/lib/kubernetes/ca.pem

Upload after local provisioning/creation.

### /var/lib/kubernetes/kubernetes-key.pem

Upload after local provisioning/creation.

###  /var/lib/kubernetes/authorization-policy.jsonl

Install on `controller` (`kube-apiserver`) nodes.
Static file, no dynamic values.

###  /var/lib/kubernetes/token.csv

Any machine with `kube-apiserver` or `kubectl`.
Includes token generated as a sort of password.

### /etc/systemd/system/kube-apiserver.service

Self IP address.
Upload auth policy to `/var/lib/kubernetes`
Upload TLS docs into `/var/lib/kubernetes`
Upload tokens file into `/var/lib/kubernetes`
Service cluster IP address range (vxlan) `10.32.0.0/24`
Service node port range via `30000-32767`
Includes list of controller IPs and port `2379`

```
https://10.240.0.8:2379,https://10.240.0.12:2379,https://10.240.0.13:2379
```

### /etc/systemd/system/kube-controller-manager.service

`scripts/start-kube-controller-manager.sh` => `/usr/local/sbin`

Self IP address.
Cluster CIDR (bridge) `10.200.0.0/16`
Service cluster IP address range (vxlan) `10.32.0.0/16`

### /etc/systemd/system/kube-scheduler.service

`scripts/start-kube-scheduler.sh` => `/usr/local/sbin`

